{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/milad/projects/End-to-End-Kidney-Disease-Classification/research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/milad/projects/End-to-End-Kidney-Disease-Classification'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    root_dir: Path\n",
    "    trained_model_path: Path\n",
    "    updated_base_model_path: Path\n",
    "    training_data: Path\n",
    "    params_epochs: int\n",
    "    params_batch_size: int\n",
    "    params_is_augmentation: bool\n",
    "    params_image_size: list\n",
    "    params_learning_rate: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnnClassifier.constants import *\n",
    "from cnnClassifier.utils.common import read_yaml, create_directories\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "    \n",
    "    \n",
    "    def get_training_config(self) -> TrainingConfig:\n",
    "        training = self.config.training\n",
    "        prepare_base_model = self.config.prepare_base_model\n",
    "        params = self.params\n",
    "        training_data = os.path.join(self.config.data_ingestion.unzip_dir, \"kidney-ct-scan-image\")\n",
    "        create_directories([\n",
    "            Path(training.root_dir)\n",
    "        ])\n",
    "        \n",
    "        training_config = TrainingConfig(\n",
    "            root_dir=Path(training.root_dir),\n",
    "            trained_model_path=Path(training.trained_model_path),\n",
    "            updated_base_model_path=Path(prepare_base_model.updated_base_model_path),\n",
    "            training_data=Path(training_data),\n",
    "            params_epochs=params.EPOCHS,\n",
    "            params_batch_size=params.BATCH_SIZE,\n",
    "            params_is_augmentation=params.AUGMENTATION,\n",
    "            params_image_size=params.IMAGE_SIZE,\n",
    "            params_learning_rate=params.LEARNING_RATE\n",
    "        )\n",
    "        \n",
    "        return training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request as request\n",
    "from zipfile import ZipFile\n",
    "import torch\n",
    "from timeit import default_timer as timer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose, ToTensor, Resize\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training_old(object):\n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        # Here we define the attributes of our class\n",
    "        self.config = config\n",
    "        self.model = self.get_base_model()\n",
    "        self.loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.config.params_learning_rate)\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        # Let's send the model to the specified device right away\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # These attributes are defined here, but since they are\n",
    "        # not informed at the moment of creation, we keep them None\n",
    "        self.train_loader = None\n",
    "        self.val_loader = None\n",
    "        \n",
    "        # These attributes are going to be computed internally\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.total_epochs = 0\n",
    "\n",
    "        # Creates the train_step function for our model, \n",
    "        # loss function and optimizer\n",
    "        # Note: there are NO ARGS there! It makes use of the class\n",
    "        # attributes directly\n",
    "        self.train_step_fn = self._make_train_step_fn()\n",
    "        # Creates the val_step function for our model and loss\n",
    "        self.val_step_fn = self._make_val_step_fn()\n",
    "\n",
    "    def get_base_model(self):\n",
    "        self.model = torch.load(\n",
    "            self.config.updated_base_model_path\n",
    "        )\n",
    "        return self.model\n",
    "    \n",
    "    \n",
    "    def to(self, device):\n",
    "        # This method allows the user to specify a different device\n",
    "        # It sets the corresponding attribute (to be used later in\n",
    "        # the mini-batches) and sends the model to the device\n",
    "        try:\n",
    "            self.device = device\n",
    "            self.model.to(self.device)\n",
    "        except RuntimeError:\n",
    "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            print(f\"Couldn't send it to {device}, sending it to {self.device} instead.\")\n",
    "            self.model.to(self.device)\n",
    "\n",
    "    def set_loaders(self):\n",
    "        # This method allows the user to define which train_loader (and val_loader, optionally) to use\n",
    "        # Both loaders are then assigned to attributes of the class\n",
    "        # So they can be referred to later\n",
    "        self.transform = Compose([Resize(16), ToTensor()])\n",
    "        self.dataset = ImageFolder(root=self.config.training_data, transform=self.transform)\n",
    "        return DataLoader(self.dataset, batch_size=self.config.params_batch_size)\n",
    "\n",
    "    def _make_train_step_fn(self):\n",
    "        # This method does not need ARGS... it can refer to\n",
    "        # the attributes: self.model, self.loss_fn and self.optimizer\n",
    "        \n",
    "        # Builds function that performs a step in the train loop\n",
    "        def perform_train_step_fn(x, y):\n",
    "            # Sets model to TRAIN mode\n",
    "            self.model.train()\n",
    "\n",
    "            # Step 1 - Computes our model's predicted output - forward pass\n",
    "            yhat = self.model(x)\n",
    "            # Step 2 - Computes the loss\n",
    "            loss = self.loss_fn(yhat, y)\n",
    "            # Step 3 - Computes gradients for both \"a\" and \"b\" parameters\n",
    "            loss.backward()\n",
    "            # Step 4 - Updates parameters using gradients and the learning rate\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Returns the loss\n",
    "            return loss.item()\n",
    "\n",
    "        # Returns the function that will be called inside the train loop\n",
    "        return perform_train_step_fn\n",
    "    \n",
    "    def _make_val_step_fn(self):\n",
    "        # Builds function that performs a step in the validation loop\n",
    "        def perform_val_step_fn(x, y):\n",
    "            # Sets model to EVAL mode\n",
    "            self.model.eval()\n",
    "\n",
    "            # Step 1 - Computes our model's predicted output - forward pass\n",
    "            yhat = self.model(x)\n",
    "            # Step 2 - Computes the loss\n",
    "            loss = self.loss_fn(yhat, y)\n",
    "            # There is no need to compute Steps 3 and 4, since we don't update parameters during evaluation\n",
    "            return loss.item()\n",
    "\n",
    "        return perform_val_step_fn\n",
    "            \n",
    "    def _mini_batch(self, validation=False):\n",
    "        # The mini-batch can be used with both loaders\n",
    "        # The argument `validation`defines which loader and \n",
    "        # corresponding step function is going to be used\n",
    "        if validation:\n",
    "            data_loader = self.val_loader\n",
    "            step_fn = self.val_step_fn\n",
    "        else:\n",
    "            data_loader = self.train_loader\n",
    "            step_fn = self.train_step_fn\n",
    "\n",
    "        if data_loader is None:\n",
    "            return None\n",
    "            \n",
    "        # Once the data loader and step function, this is the same\n",
    "        # mini-batch loop we had before\n",
    "        mini_batch_losses = []\n",
    "        for x_batch, y_batch in data_loader:\n",
    "            x_batch = x_batch.to(self.device)\n",
    "            y_batch = y_batch.to(self.device)\n",
    "\n",
    "            mini_batch_loss = step_fn(x_batch, y_batch)\n",
    "            mini_batch_losses.append(mini_batch_loss)\n",
    "\n",
    "        loss = np.mean(mini_batch_losses)\n",
    "        return loss\n",
    "\n",
    "    def set_seed(self, seed=42):\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False    \n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    def train(self, n_epochs, seed=42):\n",
    "        # To ensure reproducibility of the training process\n",
    "        self.set_seed(seed)\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            # Keeps track of the numbers of epochs\n",
    "            # by updating the corresponding attribute\n",
    "            self.total_epochs += 1\n",
    "\n",
    "            # inner loop\n",
    "            # Performs training using mini-batches\n",
    "            loss = self._mini_batch(validation=False)\n",
    "            self.losses.append(loss)\n",
    "\n",
    "            # VALIDATION\n",
    "            # no gradients in validation!\n",
    "            with torch.no_grad():\n",
    "                # Performs evaluation using mini-batches\n",
    "                val_loss = self._mini_batch(validation=True)\n",
    "                self.val_losses.append(val_loss)\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        # Builds dictionary with all elements for resuming training\n",
    "        checkpoint = {'epoch': self.config.params_epochs,\n",
    "                      'model_state_dict': self.model.state_dict(),\n",
    "                      'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                      'loss': self.losses,\n",
    "                      'val_loss': self.val_losses}\n",
    "\n",
    "        torch.save(checkpoint, self.config.trained_model_path)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        # Loads dictionary\n",
    "        checkpoint = torch.load(self.config.trained_model_path)\n",
    "\n",
    "        # Restore state for model and optimizer\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "        self.total_epochs = checkpoint['epoch']\n",
    "        self.losses = checkpoint['loss']\n",
    "        self.val_losses = checkpoint['val_loss']\n",
    "\n",
    "        self.model.train() # always use TRAIN for resuming training   \n",
    "\n",
    "    def predict(self, x):\n",
    "        # Set is to evaluation mode for predictions\n",
    "        self.model.eval() \n",
    "        # Takes aNumpy input and make it a float tensor\n",
    "        x_tensor = torch.as_tensor(x).float()\n",
    "        # Send input to device and uses model for prediction\n",
    "        y_hat_tensor = self.model(x_tensor.to(self.device))\n",
    "        # Set it back to train mode\n",
    "        self.model.train()\n",
    "        # Detaches it, brings it to CPU and back to Numpy\n",
    "        return y_hat_tensor.detach().cpu().numpy()\n",
    "\n",
    "    def plot_losses(self):\n",
    "        fig = plt.figure(figsize=(10, 4))\n",
    "        plt.plot(self.losses, label='Training Loss', c='b')\n",
    "        plt.plot(self.val_losses, label='Validation Loss', c='r')\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLazyDataset(torch.utils.Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.transform:\n",
    "            x = self.transform(self.dataset[index][0])\n",
    "        else:\n",
    "            x = self.dataset[index][0]\n",
    "        y = self.dataset[index][1]\n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "class Training(object):\n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        # Here we define the attributes of our class\n",
    "        self.config = config\n",
    "        self.model = self.get_base_model()\n",
    "        self.loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.config.params_learning_rate)\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        # Let's send the model to the specified device right away\n",
    "        self.model.to(self.device)\n",
    "    \n",
    "    def get_base_model(self):\n",
    "        self.model = torch.load(\n",
    "            self.config.updated_base_model_path\n",
    "        )\n",
    "        return self.model\n",
    "\n",
    "    def set_loaders(self):\n",
    "        # This method allows the user to define which train_loader (and val_loader, optionally) to use\n",
    "        # Both loaders are then assigned to attributes of the class\n",
    "        # So they can be referred to later\n",
    "        self.transform = Compose([Resize(16), ToTensor()])\n",
    "        self.dataset = ImageFolder(root=self.config.training_data, transform=self.transform)\n",
    "        traindataset = MyLazyDataset(self.dataset, self.transform)\n",
    "        valdataset = MyLazyDataset(self.dataset, self.transform)\n",
    "        num_workers=2\n",
    "        batch_size=6\n",
    "        trainLoader = DataLoader(traindataset , batch_size=batch_size, \n",
    "                                                num_workers=num_workers,  shuffle=True)\n",
    "        valLoader = DataLoader(valdataset, batch_size=batch_size, \n",
    "                                                num_workers=num_workers )\n",
    "        \n",
    "        return trainLoader, valLoader\n",
    "\n",
    "    def train(self, max_epochs_stop=3,print_every=1, train_on_gpu=True):\n",
    "        \"\"\"Train a PyTorch Model\n",
    "\n",
    "        Params\n",
    "        --------\n",
    "            model (PyTorch model): cnn to train\n",
    "            criterion (PyTorch loss): objective to minimize\n",
    "            optimizer (PyTorch optimizier): optimizer to compute gradients of model parameters\n",
    "            train_loader (PyTorch dataloader): training dataloader to iterate through\n",
    "            valid_loader (PyTorch dataloader): validation dataloader used for early stopping\n",
    "            save_file_name (str ending in '.pt'): file path to save the model state dict\n",
    "            max_epochs_stop (int): maximum number of epochs with no improvement in validation loss for early stopping\n",
    "            n_epochs (int): maximum number of training epochs\n",
    "            print_every (int): frequency of epochs to print training stats\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "            model (PyTorch model): trained cnn with best weights\n",
    "            history (DataFrame): history of train and validation loss and accuracy\n",
    "        \"\"\"\n",
    "        train_loader, valid_loader = self.set_loaders()\n",
    "        # Early stopping intialization\n",
    "        epochs_no_improve = 0\n",
    "        valid_loss_min = np.Inf\n",
    "\n",
    "        valid_max_acc = 0\n",
    "        history = []\n",
    "\n",
    "        # Number of epochs already trained (if using loaded in model weights)\n",
    "        try:\n",
    "            print(f'Model has been trained for: {self.model.epochs} epochs.\\n')\n",
    "        except:\n",
    "            self.model.epochs = 0\n",
    "            print(f'Starting Training from Scratch.\\n')\n",
    "\n",
    "        overall_start = timer()\n",
    "\n",
    "        # Main loop\n",
    "        for epoch in range(self.config.params_epochs):\n",
    "\n",
    "            # keep track of training and validation loss each epoch\n",
    "            train_loss = 0.0\n",
    "            valid_loss = 0.0\n",
    "\n",
    "            train_acc = 0\n",
    "            valid_acc = 0\n",
    "\n",
    "            # Set to training\n",
    "            self.model.train()\n",
    "            start = timer()\n",
    "\n",
    "            # Training loop\n",
    "            for ii, (data, target) in enumerate(train_loader):\n",
    "                # Tensors to gpu\n",
    "                if train_on_gpu:\n",
    "                    data, target = data.cuda(), target.cuda()\n",
    "\n",
    "                # Clear gradients\n",
    "                self.optimizer.zero_grad()\n",
    "                # Predicted outputs are log probabilities\n",
    "                output = self.model(data)\n",
    "\n",
    "                # Loss and backpropagation of gradients\n",
    "                loss = self.loss_fn(output, target)\n",
    "                loss.backward()\n",
    "\n",
    "                # Update the parameters\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # Track train loss by multiplying average loss by number of examples in batch\n",
    "                train_loss += loss.item() * data.size(0)\n",
    "\n",
    "                # Calculate accuracy by finding max log probability\n",
    "                _, pred = torch.max(output, dim=1)\n",
    "                correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "                # Need to convert correct tensor from int to float to average\n",
    "                accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n",
    "                # Multiply average accuracy times the number of examples in batch\n",
    "                train_acc += accuracy.item() * data.size(0)\n",
    "\n",
    "                # Track training progress\n",
    "                print(\n",
    "                    f'Epoch: {epoch}\\t{100 * (ii + 1) / len(train_loader):.2f}% complete. {timer() - start:.2f} seconds elapsed in epoch.',\n",
    "                    end='\\r')\n",
    "\n",
    "            # After training loops ends, start validation\n",
    "            else:\n",
    "                self.model.epochs += 1\n",
    "\n",
    "                # Don't need to keep track of gradients\n",
    "                with torch.no_grad():\n",
    "                    # Set to evaluation mode\n",
    "                    self.model.eval()\n",
    "\n",
    "                    # Validation loop\n",
    "                    for data, target in valid_loader:\n",
    "                        # Tensors to gpu\n",
    "                        if train_on_gpu:\n",
    "                            data, target = data.cuda(), target.cuda()\n",
    "\n",
    "                        # Forward pass\n",
    "                        output = self.model(data)\n",
    "\n",
    "                        # Validation loss\n",
    "                        loss = self.loss_fn(output, target)\n",
    "                        # Multiply average loss times the number of examples in batch\n",
    "                        valid_loss += loss.item() * data.size(0)\n",
    "\n",
    "                        # Calculate validation accuracy\n",
    "                        _, pred = torch.max(output, dim=1)\n",
    "                        correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "                        accuracy = torch.mean(\n",
    "                            correct_tensor.type(torch.FloatTensor))\n",
    "                        # Multiply average accuracy times the number of examples\n",
    "                        valid_acc += accuracy.item() * data.size(0)\n",
    "\n",
    "                    # Calculate average losses\n",
    "                    train_loss = train_loss / len(train_loader.dataset)\n",
    "                    valid_loss = valid_loss / len(valid_loader.dataset)\n",
    "\n",
    "                    # Calculate average accuracy\n",
    "                    train_acc = train_acc / len(train_loader.dataset)\n",
    "                    valid_acc = valid_acc / len(valid_loader.dataset)\n",
    "\n",
    "                    history.append([train_loss, valid_loss, train_acc, valid_acc])\n",
    "\n",
    "                    # Print training and validation results\n",
    "                    if (epoch + 1) % print_every == 0:\n",
    "                        print(\n",
    "                            f'\\nEpoch: {epoch} \\tTraining Loss: {train_loss:.4f} \\tValidation Loss: {valid_loss:.4f}'\n",
    "                        )\n",
    "                        print(\n",
    "                            f'\\t\\tTraining Accuracy: {100 * train_acc:.2f}%\\t Validation Accuracy: {100 * valid_acc:.2f}%'\n",
    "                        )\n",
    "\n",
    "                    # Save the model if validation loss decreases\n",
    "                    if valid_loss < valid_loss_min:\n",
    "                        # Save model\n",
    "                        torch.save(self.model.state_dict(), self.config.trained_model_path)\n",
    "                        # Track improvement\n",
    "                        epochs_no_improve = 0\n",
    "                        valid_loss_min = valid_loss\n",
    "                        valid_best_acc = valid_acc\n",
    "                        best_epoch = epoch\n",
    "\n",
    "                    # Otherwise increment count of epochs with no improvement\n",
    "                    else:\n",
    "                        epochs_no_improve += 1\n",
    "                        # Trigger early stopping\n",
    "                        if epochs_no_improve >= max_epochs_stop:\n",
    "                            print(\n",
    "                                f'\\nEarly Stopping! Total epochs: {epoch}. Best epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_acc:.2f}%'\n",
    "                            )\n",
    "                            total_time = timer() - overall_start\n",
    "                            print(\n",
    "                                f'{total_time:.2f} total seconds elapsed. {total_time / (epoch+1):.2f} seconds per epoch.'\n",
    "                            )\n",
    "\n",
    "                            # Load the best state dict\n",
    "                            self.model.load_state_dict(torch.load(self.config.trained_model_path))\n",
    "                            # Attach the optimizer\n",
    "                            self.model.optimizer = self.optimizer\n",
    "\n",
    "                            # Format history\n",
    "                            history = pd.DataFrame(\n",
    "                                history,\n",
    "                                columns=[\n",
    "                                    'train_loss', 'valid_loss', 'train_acc',\n",
    "                                    'valid_acc'\n",
    "                                ])\n",
    "                            return self.model, history\n",
    "\n",
    "        # Attach the optimizer\n",
    "        self.model.optimizer = self.optimizer\n",
    "        # Record overall time and print out stats\n",
    "        total_time = timer() - overall_start\n",
    "        print(\n",
    "            f'\\nBest epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_acc:.2f}%'\n",
    "        )\n",
    "        print(\n",
    "            f'{total_time:.2f} total seconds elapsed. {total_time / (epoch):.2f} seconds per epoch.'\n",
    "        )\n",
    "        # Format history\n",
    "        history = pd.DataFrame(\n",
    "            history,\n",
    "            columns=['train_loss', 'valid_loss', 'train_acc', 'valid_acc'])\n",
    "        return self.model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-18 02:18:26,585: INFO: common: yaml file: config/config.yaml loaded successfully]\n",
      "[2024-02-18 02:18:26,589: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-02-18 02:18:26,591: INFO: common: created directory at: artifacts]\n",
      "[2024-02-18 02:18:26,592: INFO: common: created directory at: artifacts/training]\n",
      "Starting Training from Scratch.\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'cuda'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m     training\u001b[38;5;241m.\u001b[39mtrain(training_config\u001b[38;5;241m.\u001b[39mparams_epochs)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[0;32mIn[38], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m     training_config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_training_config()\n\u001b[1;32m      4\u001b[0m     training \u001b[38;5;241m=\u001b[39m Training(config\u001b[38;5;241m=\u001b[39mtraining_config)\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mtraining\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[0;32mIn[37], line 85\u001b[0m, in \u001b[0;36mTraining.train\u001b[0;34m(self, max_epochs_stop, print_every, train_on_gpu)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ii, (data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;66;03m# Tensors to gpu\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m train_on_gpu:\n\u001b[0;32m---> 85\u001b[0m         data, target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcuda(), \u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m()\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# Clear gradients\u001b[39;00m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'cuda'"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    training_config = config.get_training_config()\n",
    "    training = Training(config=training_config)\n",
    "    training.train(training_config.params_epochs)\n",
    "    \n",
    "except Exception as e:\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kidney",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
